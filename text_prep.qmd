---
title: "Text Prep"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
---

The ability to produce written word is one of the major separators between us and the other great apes. And with the ability to produce it, comes the benefit of interpreting it.

## Processing Text

Before we can even begin to dive into analyzing text, we must first process the text. Processing text involves several steps that can/will be combined in various ways, depending on what we are trying to accomplish. Most of the text prep can be boiled down to the following steps:

1. Normalize

2. Word and symbol removal

3. Matrix conversion

Depending on your task, you might do all of these steps or none! Not only is every text different, but every analysis is different. Careful consideration needs to be paid when analyzing text!

### Stemming

Tense aside, are loved, love, and loving the same thing? Yes, but what if we compare the actual strings? On a string comparison side, are they the same? No. We have a string with 6, 4, and 7 characters, respectively.

What if we remove the suffixes, "ed" and "ing" -- we are left with three instances of "love"? Now we have something that is equivalent in meaning and in a string sense. This is the goal of stemming.  

```{python}
import nltk
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

love_strings = ["love", "loving", "loved", "lover"]

[stemmer.stem(word) for word in love_strings]
```

We got exactly what we expected, right? You might have noticed that "lover" did not get stemmed. Do you have any idea why? Let's think through it together. "love", "loving", and "loved" are all verbs. "lover", on the other hand, is a person who loves -- it is a noun. Martin Porter's stemming <a href="http://cs.indstate.edu/~skatam/paper.pdf">algorithm</a> works incredibly well!

Hopefully, this makes conceptual sense; however, we also need to understand why we need to do it. In a great many text-based methods, we are going to create a matrix that keeps track of every term (i.e., word) in every document -- this is known as a document-term matrix. If we know that "love", "loving", and "loved" all refer to the same thing, we want it just represented once within our document-term matrix.

Shall we take a look?

```{python}
# pip3 import scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

documents = ["I've loved you three summers now, honey, but I want them all", 
              "You're my, my, my, my lover", 
              "Tell me, tell me if you love me or not", 
              "Tryna do what lovers do, ooh", 
              "I just, I can't, I just can't be loving you no more"]

vectorizer = TfidfVectorizer()

tfidf_matrix = vectorizer.fit_transform(documents)

tfidf_matrix.toarray()

vocabulary = vectorizer.get_feature_names_out()

vocabulary
```

We can see that without stemming, we have 9 terms (things like "I", "a", and "to" get removed automatically). Let's do some stemming now:

```{python}
stemmed_docs = [stemmer.stem(word) for word in documents]
```

And now the document-term matrix:

```{python}
tfidf_matrix_stemmed = vectorizer.fit_transform(stemmed_docs)

tfidf_matrix_stemmed.toarray()

vocabulary = vectorizer.get_feature_names_out()

vocabulary
```

If we are trying to find documents that are covering similar content or talking about similar things, this document-term matrix will help to draw better conclusions, because it is clear that the first three documents are talking about the act of chewing and this document-term matrix reflects that.

### Lemmatization

Stemming is often sufficient (and most modern stemmers work pretty well on their own). Still, stemming is slightly more akin to amputating an arm with a battle ax -- it works, but it is brute force. Lemmatization is a more sophisticated approach. You might have already guessed that lemmatization will find the *lemma* of a word and since you likely know about morphology, you already know that the lemma of a word is its canonical form. A group of words that form the same idea are called a lexeme (am, be, are are all within the same lexeme). Generally, the smallest form of the word is chosen as the lemma. This is a really interesting area of linguistics, but we don't need to dive fully in.  

Instead, let's see it in action.

If we compare some "love" stuff on stemming and lemmatizing, we can see what we get:

```{python}
import spacy
 # python -m spacy download en_core_web_lg

love_strings = ["love", "loving", "loved", "lover"]

[stemmer.stem(word) for word in love_strings]

nlp = spacy.load('en_core_web_lg')

docs = list(nlp.pipe(love_strings))

lemmatized_tokens = [docs[x][0].lemma_ for x in range(len(docs))]

lemmatized_tokens
```

Absolutely nothing different. Both stemming and lemmatizing will perform the same task. The act of love is comprised of a past, present, and future tense, and chew is the lemma; lover is still seen as something else entirely.

But let's take a look at something different. If we have a string of the most lovely words, what might happen?

```{python}
lovely_string = ["lovely", "lovelier", "loveliest"]

[stemmer.stem(word) for word in lovely_string]
```

That is about as close to nonsense as we could possibly get without going into Dr. Suess mode. 

But if we try lemmatization:

```{python}
docs = list(nlp.pipe(lovely_string))

lemmatized_tokens = [docs[x][0].lemma_ for x in range(len(docs))]

lemmatized_tokens
```

We get something that starts to make sense. Now, let's try these on some actual chunks of text and see what happens.

Of course, we will need to do some cleaning on our text first:

```{python}
import nltk
from nltk.stem import WordNetLemmatizer
import pandas as pd
import pyarrow

# You will only need to do this once:

# nltk.download('wordnet')
# nltk.download('omw-1.4')

# Obs -- you'll need to import nltk.

lyrics_pd = pd.read_feather(
  '/Users/sberry5/Documents/teaching/UDA/code/lyrics_scrape_python/complete_lyrics_2025.feather'
)

lyrics_pd.lyrics = lyrics_pd.lyrics.astype(str)

lyrics_sample = lyrics_pd.sample(1)

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

lyrics_sample['lyric_lemma'] = lyrics_sample.lyrics.apply(lemmatize_text)
```

The question, then, is what do you decide to do. For my money, lemmatization does a better job and getting words down to their actual meaning.

### Stop Words

Some words do us very little good: articles, prepositions, and very high frequency words. These are all words that need to be removed. Fortunately, you don't have to do this on your own -- a great many dictionaries exist that contain words ready for removal.

```{python}
import spacy
# python -m spacy download en_core_web_lg
from spacytextblob.spacytextblob import SpacyTextBlob

nlp = spacy.load('en_core_web_lg')

nlp.add_pipe('spacytextblob')

lyrics_sample['lyrics'] = lyrics_sample['lyrics'].str.replace(
  '[0-9]+Embed.*$', '', regex=True
)

docs = list(nlp.pipe(lyrics_sample['lyrics']))

# Stop words
docs[0][1].is_stop
[token.text for token in docs[0] if not token.is_stop]
docs[0][1].is_space

# Together
[token.lemma_ for token in docs[0] if not token.is_stop and not token.is_space and not token.is_punct]
```


# Initial Analyses

Like every analysis you will ever do, it is easy to try jumping right into the most complex questions you can answer with text -- and it is never the right thing to do. Text gives us the ability to do a lot of exploratory data analysis, so let's start there.

Let's start by finding a little bit of text. There is a lot out there, but let's grab some "interesting" song lyrics. 

And there you have a #1 Country Song from just a few years ago.

```{python}
from bs4 import BeautifulSoup
import pandas as pd
import requests

hfl_request = requests.get('https://genius.com/Luke-bryan-huntin-fishin-and-lovin-every-day-lyrics')

hfl_content = BeautifulSoup(hfl_request.content, 'html.parser') 

hfl_lyrics = hfl_content.select('#lyrics-root')

hfl_list = []

for i in range(len(hfl_lyrics)):
    hfl_list.append(hfl_lyrics[i].getText())

hfl_pd = pd.DataFrame([hfl_list], columns = ['lyrics'])

hfl_pd['lyrics'] = (
  hfl_pd.lyrics.str.replace('(\\[.*?\\])', '') 
  .str.replace('([a-z])([A-Z])', '\\1 \\2') 
)

```

For those that might like a little more grit to their Country, let's look at another song:

```{python}
copperhead_request = requests.get('https://genius.com/Steve-earle-copperhead-road-lyrics')

copperhead_content = BeautifulSoup(copperhead_request.content, 'html.parser') 

copperhead_lyrics = copperhead_content.select('#lyrics-root')

copperhead_list = []

for i in range(len(copperhead_lyrics)):
    copperhead_list.append(copperhead_lyrics[i].getText())

copperhead_pd = pd.DataFrame([copperhead_list], columns = ['lyrics'])

copperhead_pd['lyrics'] = (
  copperhead_pd.lyrics.str.replace('(\\[.*?\\])', '') 
  .str.replace('([a-z])([A-Z])', '\\1 \\2') 
)
```

And here is some more underground country:

```{python}
choctaw_request = requests.get('https://genius.com/James-mcmurtry-choctaw-bingo-lyrics')

choctaw_content = BeautifulSoup(choctaw_request.content, 'html.parser') 

choctaw_lyrics = choctaw_content.select('#lyrics-root')

choctaw_list = []

for i in range(len(choctaw_lyrics)):
    choctaw_list.append(choctaw_lyrics[i].getText())

choctaw_pd = pd.DataFrame([choctaw_list], columns = ['lyrics'])

choctaw_pd['lyrics'] = (
  choctaw_pd.lyrics.str.replace('(\\[.*?\\])', '') 
  .str.replace('([a-z])([A-Z])', '\\1 \\2') 
)
```

We clearly have very different songs: one about living the outlaw life, one about living the "country-bro" life, and one about your typical American family reunion. From here on, it might be worth exploring more about these three types of songs.

```{python}
song_data = pd.concat([hfl_pd, copperhead_pd, choctaw_pd], axis = 0)  
```

## Term Frequency

Just like any other data, text has some basic descriptives, with term frequency (tf -- $f_{t,d}$) being incredibly useful. When we are looking at term frequency, we are looking for a few different words: high and low frequency. If a word is high frequency (think: "the"), then it might not really be offering us much in the way of anything informative. Likewise, a word that only occurs once or twice might not be terribly important either. 

We can calculate term frequency (adjusted for for document length) as the following:

$$tf=\frac{N_{term}}{Total_{terms}}$$

When looking at a corpus, it is important to adjust for the length of the text when calculating term frequency (naturally, longer texts will have words occurring more frequently). 

There are a few other ways of calculating term frequency:

A raw weight is depicted as $f_{t,d}$ -- the frequency with which *t* (the term) is found in *d* (the document)

If you want to effectively normalize huge numbers and minimize the differences between huge numbers, $log(1+f_{t,d})$

If you have huge differences in document length, you might use augmented term frequency: $k + (1-k)\frac{tf}{max(t,f)}$, where *k* helps to mitigate the effects of document length (it essentially removes the bias towards longer documents).

This <a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.04007.pdf">article</a> has a nice run down of more methods.


## Inverse Document Frequency

We can know how many times any word was used within a text when we look at our term frequencies. Inverse document frequency (IDF) gives us something a little bit different. If a word is incredibly common, it might not be very important to a document; however, rare words might be important within our documents. To that end, we would assign a higher weight to words that occur less frequently than words that are common.

We can calculate idf as the natural log of the number of the number of documents divided by the number of documents containing the term. We really don't need any fancy functions to make that calculation -- we can just do it by hand in a mutate function.

Our idf is just telling us what we need to know about the corpus-wide term counts. We can see that words that appear in all three of our songs have a very low idf, while words that appear in only one song have a much higher idf.

## tf-idf

After considering the two in isolation, we can also consider what both of them will get for us together. If we take the term frequency to mean that words are appearing frequently within our text and we take our inverse document frequency to mean that we are only considering important words, we might imagine a set of words appearing commonly within a document, but not appearing within other documents as often. This would suggest high-weight words for a specific document. 

It can be tempting to just cut stop words out and deal with everything that comes out -- this is not the place for that. Stopword removal, for all practical purposes, is brute force. If we want to have a bit of finesse here, we want to leave open the possibility that words, even potentially common words within a document, can have different levels of importance across documents.

```{python}
tfidf_vec = TfidfVectorizer()

songs_tfidf = tfidf_vec.fit_transform(song_data['lyrics'])

tfidf_tokens = tfidf_vec.get_feature_names_out()

df_countvect = pd.DataFrame(data = songs_tfidf.toarray(), 
  columns = tfidf_tokens)
```


### Practice Time

Let's return to those last statements:

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from random import sample

link_html = requests.get(
  'https://www.tdcj.texas.gov/death_row/dr_executed_offenders.html', 
  verify=False
  )

link_content = BeautifulSoup(link_html.content, 'html.parser')

link_list = link_content.select('a[href*="last"]')

link_list = [link_list[i].get('href') for i in range(len(link_list))]

link_list = ['https://www.tdcj.texas.gov/death_row/'+link_list[i] for i in range(len(link_list))]

link_list = [link_list[i].replace('//death_row', '') for i in range(len(link_list))]

link_list = [link_list[i] for i in range(len(link_list)) if 'no_last_statement' not in link_list[i]]

link_list = sample(link_list, 50)

for i in range(len(link_list)):
    link_html = requests.get(link_list[i], verify=False)
    link_content = BeautifulSoup(link_html.content, 'html.parser')
    link_list[i] = link_content.select('p:contains("Last Statement:")~*')
    link_list[i] = [link_list[i][j].getText() for j in range(len(link_list[i]))]
    link_list[i] = ' '.join(link_list[i])
    
```


## String Distance And Similarity

Now that we know a little bit about n-grams (and individual words), we can talk about string distances. If you ever need to know how similar (or dissimilar) two words/texts are, then string distances are what you need. But...which one should we use. 

### Levenshtein

This is probably the most common string distance metric you will see (it is pretty common in genetics research, among other areas). Conceptually, it is pretty easy -- we are just finding the number of changes that need to be made to one string to equal another string.

Let's look at two names:

```{python}
import textdistance

# install as TextDistance

textdistance.levenshtein('bono', 'gaga')
```

To transform "bono" into "gaga", we would need to replace the "b" with a "g", the "o" with an "a", the "n" with a "g", and the "o" with an "a" -- all leading to a Levenshtein distance of 4. We can also look at the similarity between the two:

```{python}
textdistance.levenshtein.similarity('bono', 'gaga')
```

As to be expected. The similarity is computed as the string distance score, divide it by the maximum feasible distance, and then subtract from 1. 

Those are clearly different words, but what about something a little closer together?

```{python}
textdistance.levenshtein('beauty', 'beautiful')
```

Still 4. That is the tricky thing with Levenshtein distance -- string length matters.  

Let's check the similarity now:

```{python}
textdistance.levenshtein.normalized_similarity('beauty', 'beautiful')
```

We have our distance (4), divided by the max possible distance (beautiful has 9 letters, so 4 / 9 = .4444444), and subtract that from 1 (1 - .4444444 = .5555556). 

The similarity here is a bit more telling than our distance. 

### Jaccard

The Jaccard Index is an extremely flexible metric that goes even beyond strings (it is used in computer vision, pure mathematics, and various other places). It is most useful when comparing sets as opposed to just words. 

We can also try it with different values of *q* to really get a feel for what is happening:

```{python}
textdistance.jaccard.distance('soup can', 'soup tin')
textdistance.jaccard.similarity('soup can', 'soup tin')
```

No matter the language you can also use the Jaro-Winkler metric. It provides a "score bump" for strings that start with the same letters.

```{python}
textdistance.jaro_winkler.distance('soup can', 'soup tin')
textdistance.jaro_winkler.similarity('soup can', 'soup tin')
```

String distances can be handy for a great many tasks. If you want to find strings that are close to other strings (without being exact matches), then these distances can be useful. They can also be helpful when you want to join data frames with fields that might not match.  
